---
id: "financial-research-assistant-rag"
title: "Financial Research Assistant (RAG)"
summary: "Production-ready local RAG system powered by Microsoft Phi-3 Mini and BGE embeddings. Features GPU-accelerated GGUF inference, sub-second retrieval, and citation-backed financial analysis."
category: "Deep Learning"
coverImage: "/images/projects/financial-rag/cover.png"
date: "2025-12-28"
techStack:
  - "Python"
  - "FastAPI"
  - "Streamlit"
  - "Microsoft Phi-3"
  - "LlamaCpp"
  - "LangChain"
  - "FAISS"
  - "CUDA"
featured: true
status: "Completed"
metrics:
  - label: "Retrieval Speed"
    value: "< 100ms"
  - label: "GPU VRAM"
    value: "< 4GB"
  - label: "Quantization"
    value: "4-bit (GGUF)"
  - label: "Model"
    value: "Phi-3 Mini 3.8B"
githubUrl: "https://github.com/HimanshuSalunke/Financial-Research-Assistant-RAG-"
liveUrl: ""
publishedAt: "2025-12-28"
order: 0
completionDate: "2025-12-27"
images: []
link: "https://github.com/HimanshuSalunke/Financial-Research-Assistant-RAG-"
outputLink:
tag: "Generative AI"
---



## ðŸŽ¯ Project Overview

Financial analysts spend countless hours manually searching through dense 10-K reports and financial statements. This upgraded **Financial Research Assistant** solves this by leveraging state-of-the-art **Small Language Models (SLMs)** to provide instant, accurate answers running entirely on local hardware.

**The Upgrade:** Moved from generic embeddings to **BAAI/bge-small-en-v1.5** (MTEB leaderboard top-performer) and from standard LLMs to **Microsoft Phi-3 Mini 4K Instruct**, running via **llama.cpp** for optimized GPU inference.

## System Architecture

```mermaid
graph TD
    A[PDF Financial Docs] -->|PyPDF Loader| B(Text Chunks 1200 chars)
    B -->|Recursive Splitter| C[Chunk Processing]
    C -->|BAAI/bge-small-en-v1.5| D[Vector Embeddings]
    D -->|FAISS| E[(Vector Database)]
    E <-->|Similarity Search| F[RAG Retrieval Chain]
    G[User Query] -->|BGE Embedding| F
    F -->|Context + Query| H[Phi-3 Mini 4K (GGUF)]
    H -->|LlamaCpp via CUDA| I[Final Response]
```

## Key Achievements

- **State-of-the-Art Local Inference**: Implemented **Microsoft Phi-3 Mini** (3.8B parameters) using 4-bit GGUF quantization, achieving GPT-3.5 level performance on reasoning tasks while running locally on an RTX 4050.

- **High-Precision Retrieval**: Upgraded to **BAAI/bge-small-en-v1.5** embeddings, significantly significantly improving semantic search accuracy compared to standard BERT models.

- **Full GPU Acceleration**: Integrated `llama-cpp-python` with CUDA support to offload 100% of model layers to the GPU, enabling blazing fast token generation.

## Technical Stack
- **AI Core**: Microsoft Phi-3 Mini, LlamaCpp, HuggingFace Transformers
- **Backend**: FastAPI with Pydantic validation
- **Frontend**: Streamlit for interactive chat and document visualization
- **Vector Store**: FAISS (Facebook AI Similarity Search)

## Technical Implementation

### RAG Engine Initialization
```python
class RAGEngine:
    def _initialize(self):
        # 1. Load SOTA Embeddings (BGE) on GPU
        device = "cuda" if torch.cuda.is_available() else "cpu"
        embeddings = SentenceTransformerEmbeddings(
            model_name="BAAI/bge-small-en-v1.5",
            device=device
        )
        
        # 2. Initialize Phi-3 via LlamaCpp (GGUF)
        self.llm = LlamaCpp(
            model_path="models/Phi-3-mini-4k-instruct-q4.gguf",
            n_gpu_layers=-1, # Offload ALL layers to GPU
            n_ctx=4096,      # Full 4K context window
            f16_kv=True,     # FP16 KV cache for speed
            temperature=0.1  # Low temp for factual answers
        )
```

### Advanced Prompt Engineering
Used strict system Prompting to reduce hallucinations:
```python
prompt_template = """<|system|>
You are a financial research assistant. Answer the question specifically using ONLY the provided context.
If the answer is not in the context, say "I cannot find the answer in the documents."
Context:
{context}
<|end|>
<|user|>
{question}
<|end|>
<|assistant|>"""
```

## Challenges & Learnings
- **Quantization Trade-offs**: Balacing model size vs. perplexity. Selected **Q4_K_M** quantization for Phi-3 as the sweet spot for 6GB VRAM GPUs.
- **Context Window Management**: Phi-3 supports 4K tokens, which is crucial for analyzing long financial narratives. Implemented smart chunking (1200 chars) to maximize context utilization.
